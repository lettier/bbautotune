\chapter{Genetic Algorithms}

%\todo{Most of this could be filled out by the information in the SimPL chapter with maybe some additional bits from the BBAutoTune chapter to flesh it out.}

\label{Chapter2}

\section{Overview}

GAs have been applied to various problem domains since the works of John Holland, Hans-Paul Schwefel, Ingo Rechenberg, Lawrence Fogel, and John Koza \cite{ColinReeves}\cite{Michalewicz}. There is no guarantee that any two GAs are the same but the central idea is to use a GA to find the globally optimal solution to some multivariate state space. All of the possible states in the state space make up a fitness landscape where one state is more optimal or fitter than another. The goal of the GA is to find the global maximum or minimum point on the fitness landscape. Intuitively, one can think of the fitness landscape as a foggy mountainousness terrain. As the GA progresses, more and more of this landscape is explored as the GA searches for the highest or maybe the lowest point in the terrain depending on the particular problem the GA has been tasked with. A GA may never find this global maxima or minima but may converge to some local optima depending on the GA's implementation and the fitness landscape. Coinciding with the exploration of the fitness landscape is the concept of mutation and crossover. Using both crossover and mutation, the GA can either expand or narrow its search of the fitness landscape. 

All GAs have the concept of a population where each member in the population represents a point on the fitness landscape. Initially, a GA's population is usually randomized but could be seeded with some known-to-be \textit{good} solutions by the GA's designer \cite{DBLP:conf/gem/Diaz-GomezH07}. As the GA runs, new populations are formed from previous populations where each population instance is known as a generation. When to stop the GA from producing new generations, known as termination, is up to the designer. Some common cases for termination include GA run time or population diversity (how different each population member is from one another) dropping below some threshold \cite{ColinReeves}.             

\section{Genomes}

Taking a page from nature, GAs have the concept of genomes (also called chromosomes in some texts). Genomes make up the GA's population at any given time and each represents a possible solution (or possible partial solution) to the problem the GA is attempting to solve or rather find an optimum solution to. Various data structures ranging from strings to classes can be used to construct a genome \cite{ColinReeves}. However, at the heart of every genome is its genes or vector of variables where each variable corresponds to a dimension in the multidimensional state space the GA is searching.      

\subsection{Genes}

The genome as a whole represents a state in some multivariate state space inherent to the problem the GA is attempting to solve. Collectively, the genes of a genome describe a genotype while the structure (hardware and/or software) and behavior they produce is known as the phenotype. A genotype is evaluated by its phenotype's performance \cite{Beasley93anoverview}. Imagine the genes of a genome being the blueprint of a motorcycle. This blueprint would be the genotype while the motorcycle built by following the blueprint would be the phenotype. If the goal was to produce the fastest motorcycle on earth, each blueprint's score would correlate to its motorcycle's maximum speed. Depending on the encoding the designer has chosen, each gene may be a bit, string, real number, natural number, or some other datum such as a function or s-expression in the case of genetic programming \cite{DBLP:conf/aaai/KozaR92}. Furthermore, not all the genes have to be of the same type but collectively could be some mixed tuple of various data types \cite{ColinReeves}.      

\subsubsection{Encoding}

How the variables to a problem are encoded into genes is entirely problem dependent and up to the designer. Typical encoding schemes include:
\begin{enumerate}
 \item binary---where every gene is a bit or a string of bits;
 \item real-coded---where the genes could be real numbers, integers, or character strings;
 \item tree---where each gene is a node in a tree that when parsed, forms some kind of expression; and
 \item permutation---where the genes are some possible order of a sequence \cite{encoding_schemes}.
\end{enumerate}
There is no hard and fast rule governing the encoding of genes. Some have claimed that a binary encoding is best but this notion has been contested \cite{ColinReeves}\cite{Herrera}. For problem domains with variables in $\mathbb{R}$, precision can be an issue when choosing a binary encoding. Another issue arises during the binary encoding of a finite set of discrete values when the set cardinality is not a power of two \cite{Herrera}. For example, image a variable that has five possible discrete values. To represent the five possible values, a three bit gene would be required. However, the gene configurations $101$, $110$, and $111$ would be redundant or invalid and would need to be handled in the reproduction operators \cite{Whitley94agenetic}.     

In \cite{Whitley94agenetic}, Holland's GA model is described as \textit{the conical genetic algorithm}. Binary encoded/coded GAs (BCGA) that use recombination and selection follow Holland's model. A BCGA's search space is $\{0,1\}^l$. For one-bit genes, each gene represents the act of turning a variable on or off for some multivariate state space. Another interpretation of one-bit genes is that the bit represents the presence or absence of a phenotype trait---an analogy being a phenotype having or not having eyes. However, each gene can be made up of more than bit. In this case, each gene represents the magnitude of some variable or a set of discrete values a variable can be.

Parallel to Holland's work, Hans-Paul Schwefel and Ingo Rechenberg developed the concept of \textit{evolution strategy} (ES) in the 1960's \cite{ColinReeves}. Originally, ES only used mutation and its population size was one \cite{Michalewicz}. However, other recombination operators and population sizes were later considered as ES researched progressed \cite{Herrera}. ES employs a real-coded scheme where each population member (genome) contains two vectors of floating point values. The first vector of floats is a point in the search space and the second vector of floats is a vector of standard deviations for use in the mutation operator \cite{Herrera}. For some population member, let $\vec{\kappa}=\langle\kappa_1,\kappa_2,...,\kappa_n\rangle$ be the vector of floats representing a point in the search space and let $\vec{\sigma}=\langle\sigma_1,\sigma_2,...,\sigma_n\rangle$ be the corresponding vector of standard deviations. Here, a mutant offspring's $\vec{\kappa}'=\vec{\kappa}+N(0,\vec{\sigma})$ and its $\vec{\sigma}'=\vec{\sigma}$ after mutation where $N(0,\vec{\sigma})$ is a vector of samples from a normal distribution with a mean of zero and a standard deviation $\sigma_i \in \vec{\sigma}$ \cite{Michalewicz}. This mutant offspring only becomes a population member (by replacing its parent) if it obtains a higher fitness than its parent \cite{Michalewicz}. One could think of this strategy as being a probing of the fitness landscape, where population members never move from their locations until their random probes discover more optimal locales.

Another paradigm of evolutionary computing, differentiated mainly by its encoding scheme, is genetic programming (GP). GP was developed by John Koza \cite{Michalewicz}. In 1990, Koza presented the concept in his paper, \textit{Genetic Programming: A Paradigm for Genetically Breeding Populations of Computer Programs to Solve Problems}. Instead of searching for a solution to a problem directly, GP uses evolutionary tactics to search for a best-fit computer program capable of solving the problem at hand on its own. For example, in \cite{DBLP:conf/aaai/KozaR92}, Koza uses GP to evolve a computer program that dictates the behavior of an autonomous robot where the evolutionary goal was to get the robot to push a box from the center of a room to a wall. The main GP operator is crossover but mutation is used as well \cite{Michalewicz}. Each population member is a rooted tree, that when parsed, represents a hierarchical computer program. ``Internal points of the tree correspond to functions (i.e. operations) and external points correspond to terminals (i.e. input data)'' \cite{DBLP:conf/aaai/KozaR92}. 

\section{Evaluation}

Evaluation of each phenotype yields the fitness of the corresponding genotype. Besides the encoding scheme, the fitness function is most critical aspect to get \textit{right} when developing a GA. Fitness functions that are binary, noisy, computationally expensive, erratic, or discontinuous can produce poor results on behalf of a GA \cite{Beasley93anoverview}. If at all possible, the fitness function should have a gradual gradient and a minimal number of local optima \cite{Beasley93anoverview}.     

\subsection{Criteria}

The criteria for evaluation usually lends itself when one envisions what goals a perfect solution would meet. For certain problem domains, the criteria for evaluation may include constraints where a genome is penalized for some constraint violated \cite{Beasley93anoverview}. Some implementations incorporate constraint adherence within the reproduction operators, so that it is not possible to produce an invalid phenotype.

\subsection{Fitness Function}

Constructing the right fitness function for the problem at hand is paramount when developing a GA. Based on the problem and the way in which the fitness function has been constructed, the GA's task usually boils down to one of either minimizing or maximizing the fitness function \cite{ColinReeves}. For problems with a unique solution where a genome is either completely correct or completely wrong with no leeway in between, one may want to include subgoals or partial credit for a genome that only provides a partial solution \cite{Beasley93anoverview}. An analogy would be a professor giving partial credit for an answer on an exam. 

Generally, the fitness function should always provide some new knowledge of how well one genome performed in comparison to others, where each fitness value outputted logically and intuitively corresponds to the problem \cite{Beasley93anoverview}. If, after evaluating an entire population, all genomes have the same fitness value even though they all provide varying proposed solutions, then the GA has gained no new knowledge from which it can capitalize on as it constructs a new generation.      

\section{Operators}

The tools of the GA are its operators. Many variants exist but the general operator categories are selection, elitism, crossover, and mutation. Crossover and mutation play off of another another by adding a delicate mix of exploitation versus exploration \cite{Beasley93anoverview}. Elitism attempts to preserve the best solutions found so far when generating a new population. Mimicking natural selection, the selection operator provides each population member with some opportunity (however small) at passing its genes on to the next generation \cite{Beasley93anoverview}.      

\subsection{Selection}

The general concept behind selection is to weight the probability of being selected for reproduction towards fitter genomes. Roulette wheel, rank, and tournament selection are some of the most well-known selection methods. All have been studied in depth, with each method having its own set of parameters. Some researchers have conjectured that by tweaking the parameters of each method, all can be constructed to have similar performance and thus no method is superior over another \cite{Beasley93anoverview}.    

\subsection{Elitism}

Elitism is one of the more simple operators. After the entire population has been evaluated, the top $e$ performers are directly copied into the new population during the reproduction cycle. In addition to being directly copied into the next generation, the $e$ elites could also be crossed and/or mutated. Whether or not the elites are also considered during selection is up to the designer. Deciding on the size of $e$---in relation to the population size---becomes a delicate balance between premature convergence and the GA losing information about the best solutions found so far thereby forcing it to possibly run longer relearning what it already learned previously \cite{DanWDyer}. Note that $e$ does not necessarily remain constant but could be varied per generation according to the number (or percentage) of population members that exceed a certain fitness threshold. 

\subsection{Crossover}

Crossover employs the notion of exploitation where knowledge gained about points on the fitness landscape is capitalized on to find better points. Exploitation differentiates GAs from random search \cite{Beasley93anoverview}. Selecting two genomes as parents, the crossover operator produces one or more offspring such that the offspring have some genes from one parent and some genes from parent two. Typical variants of crossover are $1$-point crossover, $m$-point crossover, and uniform crossover where the offspring receives a random number of genes from parent one and a random number of genes from parent two \cite{ColinReeves}. Note that crossover does not always occur after selection but rather occurs based on some probability known as the crossover probability.    

\subsection{Mutation}

Mutation is the exploration component of a GA where mutant offspring may discover previously uncharted points on the fitness landscape \cite{Beasley93anoverview}. Coupled with a mutation step, the mutation operator disperses current genomes in the population to random points on the fitness landscape like dandelion seeds blowing in the wind; the larger the mutation step, the greater the dispersion. Like most aspects of a GA, there are many mutation operator variants depending on what gene encoding scheme was employed. Central to all variants, however, is some random number generator. 

Mutation can occur on a gene-by-gene or on a genome-by-genome basis. With some probability of occurring---known as the mutation probability---a genome's genes will be mutated (randomly altered) thereby producing a mutant offspring. These mutant offspring allow a GA to escape local optimums and also help to keep the population diverse \cite{ColinReeves}. As the GA progresses, the diversity of the population will ultimately depend on the mutation probability, but by employing some self-adaptation mechanism, the mutation probability can vary over time \cite{ColinReeves}. Note that, like the mutation probability, the crossover probability could also be self-adapted over time as well.       

\section{Population}

\subsection{Initialization}

Typically, at the start of a GA run, the population is randomly generated, but it could be seeded with some high-performing genomes found by an earlier run or by some other mechanism \cite{DBLP:conf/gem/Diaz-GomezH07}\cite{ColinReeves}. However, if one does seed the initial population, this may lead to premature convergence \cite{ColinReeves}. Determining the population size, and whether or not the population size remains constant throughout the run of the GA, is a debated topic with no straightforward answers \cite{ColinReeves}.     

\subsection{Reproduction}

Once every genome in the population has been evaluated, a GA enters into its reproduction cycle producing a new population known as a generation. There are many approaches to the reproduction cycle, but the most basic is performing selection, crossover, and then mutation in a loop until the new population size reaches the old population size \cite{Beasley93anoverview}. This basic approach is known as the generational approach with a generational gap of one between each generation. The generational gap is how much of an old population is replaced by a new population. A generational gap of one indicates that the new generation entirely replaces the old generation or in other words, parents never co-exist with offspring during any given generation \cite{Beasley93anoverview}. Another approach is the steady-state or incremental approach where the generational gap is greater than one, with only a few offspring being produced and a few population members being terminated during each reproduction cycle \cite{ColinReeves}. Human beings follow a steady-state approach with some dying and some coexisting with their children, after having reproduced.

\subsection{Convergence}

Population diversity coincides with population convergence. Gene convergence occurs when $95\%$ of the population shares the same gene value while population convergence occurs when every gene in the population has converged \cite{Beasley93anoverview}. A converged population will be minimally diverse at the gene, genotype, and phenotype level \cite{ColinReeves}.

\section{Termination}

GAs are inherently a stochastic process and will run forever unless constructed otherwise \cite{ColinReeves}. Deciding when to terminate is entirely up to the designer and their needs. The simplest method of termination is time where the GA is run for some amount of time and then stopped. A more sophisticated method involves terminating a GA after the population has dropped below some threshold of diversity \cite{ColinReeves}. Additional methods of when to terminate include:
\begin{enumerate}
 \item the highest or average fitness reaching some predefined metric;
 \item the change in fitness from one generation to another falling below some threshold; and
 \item the GA reaching some generation number.
\end{enumerate}